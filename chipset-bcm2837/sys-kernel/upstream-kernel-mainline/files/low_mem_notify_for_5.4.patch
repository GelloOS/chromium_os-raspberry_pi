diff --git a/drivers/char/mem.c b/drivers/char/mem.c
index 31cae88a730b..ef3c65a14361 100644
--- a/drivers/char/mem.c
+++ b/drivers/char/mem.c
@@ -34,6 +34,7 @@
 #include <linux/pseudo_fs.h>
 #include <uapi/linux/magic.h>
 #include <linux/mount.h>
+#include <linux/low-mem-notify.h>
 
 #ifdef CONFIG_IA64
 # include <linux/efi.h>
@@ -957,6 +958,9 @@ static const struct memdev {
 #ifdef CONFIG_PRINTK
 	[11] = { "kmsg", 0644, &kmsg_fops, 0 },
 #endif
+#ifdef CONFIG_LOW_MEM_NOTIFY
+  [12] = { "chromeos-low-mem", 0666, &low_mem_notify_fops, 0 },
+#endif
 };
 
 static int memory_open(struct inode *inode, struct file *filp)
diff --git a/drivers/gpu/drm/v3d/v3d_drv.c b/drivers/gpu/drm/v3d/v3d_drv.c
index 91b0769b69ef..47b29ee1d253 100644
--- a/drivers/gpu/drm/v3d/v3d_drv.c
+++ b/drivers/gpu/drm/v3d/v3d_drv.c
@@ -175,7 +175,7 @@ DEFINE_DRM_GEM_SHMEM_FOPS(v3d_drm_fops);
  * need to stay DRM_AUTH until we do buffer size/offset validation.
  */
 static const struct drm_ioctl_desc v3d_drm_ioctls[] = {
-	DRM_IOCTL_DEF_DRV(V3D_SUBMIT_CL, v3d_submit_cl_ioctl, DRM_RENDER_ALLOW | DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(V3D_SUBMIT_CL, v3d_submit_cl_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(V3D_WAIT_BO, v3d_wait_bo_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(V3D_CREATE_BO, v3d_create_bo_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(V3D_MMAP_BO, v3d_mmap_bo_ioctl, DRM_RENDER_ALLOW),
diff --git a/drivers/gpu/drm/vc4/vc4_bo.c b/drivers/gpu/drm/vc4/vc4_bo.c
index 72d30d90b856..c7b0ee9d85ce 100644
--- a/drivers/gpu/drm/vc4/vc4_bo.c
+++ b/drivers/gpu/drm/vc4/vc4_bo.c
@@ -820,6 +820,7 @@ int vc4_create_bo_ioctl(struct drm_device *dev, void *data,
 	int ret;
 
 	ret = vc4_grab_bin_bo(vc4, vc4file);
+  ret = 0;
 	if (ret)
 		return ret;
 
@@ -885,6 +886,7 @@ vc4_create_shader_bo_ioctl(struct drm_device *dev, void *data,
 	}
 
 	ret = vc4_grab_bin_bo(vc4, vc4file);
+  ret = 0;
 	if (ret)
 		return ret;
 
diff --git a/include/linux/low-mem-notify.h b/include/linux/low-mem-notify.h
new file mode 100644
index 000000000000..0c2c18a9cc39
--- /dev/null
+++ b/include/linux/low-mem-notify.h
@@ -0,0 +1,22 @@
+#ifndef _LINUX_LOW_MEM_NOTIFY_H
+#define _LINUX_LOW_MEM_NOTIFY_H
+
+#include <linux/types.h>
+
+#ifdef CONFIG_LOW_MEM_NOTIFY
+extern const struct file_operations low_mem_notify_fops;
+
+void low_mem_notify(void);
+bool low_mem_check(void);
+#else
+static inline void low_mem_notify(void)
+{
+}
+
+static inline bool low_mem_check(void)
+{
+	return false;
+}
+#endif
+
+#endif
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 3285dae06c03..3c1b9fc0079d 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2901,5 +2901,7 @@ static inline int pages_identical(struct page *page1, struct page *page2)
 	return !memcmp_pages(page1, page2);
 }
 
+extern int min_filelist_kbytes;
+
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MM_H */
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 70665934d53e..e09f8fdf9588 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1708,6 +1708,13 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_doulongvec_minmax,
 	},
+  {
+    .procname = "min_filelist_kbytes",
+    .data   = &min_filelist_kbytes,
+    .maxlen   = sizeof(min_filelist_kbytes),
+    .mode   = 0644,
+    .proc_handler = proc_dointvec,
+  },
 #ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS
 	{
 		.procname	= "mmap_rnd_bits",
diff --git a/mm/Kconfig b/mm/Kconfig
index a5dae9a7eb51..c9c01cdb1915 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -736,4 +736,15 @@ config ARCH_HAS_PTE_SPECIAL
 config ARCH_HAS_HUGEPD
 	bool
 
+config LOW_MEM_NOTIFY
+  bool "Create device that lets processes detect low-memory conditions"
+  default n
+  help
+    A process can poll the /dev/low_mem device to be notified of
+    low-memory conditions.  The process can then attempt to free memory
+    before a OOM condition develops and the OOM killer takes over.  This
+    is meant to be used in systems with no or very little swap space.  In
+    the presence of large swap space, the system is likely to become
+    unusable before the OOM killer is triggered.
+
 endmenu
diff --git a/mm/Makefile b/mm/Makefile
index d996846697ef..a0f3d5ac7be8 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -101,6 +101,7 @@ obj-$(CONFIG_CMA_DEBUGFS) += cma_debug.o
 obj-$(CONFIG_USERFAULTFD) += userfaultfd.o
 obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o
 obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o
+obj-$(CONFIG_LOW_MEM_NOTIFY) += low-mem-notify.o
 obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
 obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
diff --git a/mm/low-mem-notify.c b/mm/low-mem-notify.c
new file mode 100644
index 000000000000..83b697ba9583
--- /dev/null
+++ b/mm/low-mem-notify.c
@@ -0,0 +1,396 @@
+/*
+ * mm/low-mem-notify.c
+ *
+ * Sends low-memory notifications to processes via /dev/low-mem.
+ *
+ * Copyright (C) 2012 The Chromium OS Authors
+ * This program is free software, released under the GPL.
+ * Based on a proposal by Minchan Kim
+ *
+ * A process that polls /dev/low-mem is notified of a low-memory situation.
+ * The intent is to allow the process to free some memory before the OOM killer
+ * is invoked.
+ *
+ * A low-memory condition is estimated by subtracting anonymous memory
+ * (i.e. process data segments), kernel memory, and a fixed amount of
+ * file-backed memory from total memory.  This is just a heuristic, as in
+ * general we don't know how much memory can be reclaimed before we try to
+ * reclaim it, and that's too expensive or too late.
+ *
+ * This is tailored to Chromium OS, where a single program (the browser)
+ * controls most of the memory, and (currently) no swap space is used.
+ */
+
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/ctype.h>
+#include <linux/ratelimit.h>
+#include <linux/stddef.h>
+#include <linux/swap.h>
+
+#define MB (1 << 20)
+
+static DECLARE_WAIT_QUEUE_HEAD(low_mem_wait);
+static atomic_t low_mem_state = ATOMIC_INIT(0);
+
+/* We support up to this many different thresholds. */
+#define LOW_MEM_THRESHOLD_MAX 5
+
+/* This is a list of thresholds in pages and should be in ascending order. */
+static unsigned long low_mem_thresholds[LOW_MEM_THRESHOLD_MAX] = {
+	24 * MB / PAGE_SIZE
+};
+static unsigned int low_mem_threshold_count = 1;
+
+static bool low_mem_margin_enabled = true;
+static unsigned int low_mem_ram_vs_swap_weight = 4;
+
+void low_mem_notify(void)
+{
+	atomic_set(&low_mem_state, true);
+	wake_up(&low_mem_wait);
+}
+
+/*
+ * Compute available memory used by files that can be reclaimed quickly.
+ */
+static unsigned long get_available_file_mem(void)
+{
+	unsigned long file_mem =
+			global_node_page_state(NR_ACTIVE_FILE) +
+			global_node_page_state(NR_INACTIVE_FILE);
+	unsigned long dirty_mem = global_node_page_state(NR_FILE_DIRTY);
+	unsigned long min_file_mem = min_filelist_kbytes >> (PAGE_SHIFT - 10);
+	unsigned long clean_file_mem = file_mem > dirty_mem ?
+			file_mem - dirty_mem : 0;
+	/* Conservatively estimate the amount of available_file_mem */
+	unsigned long available_file_mem = clean_file_mem > min_file_mem ?
+			clean_file_mem - min_file_mem : 0;
+	return available_file_mem;
+}
+
+/*
+ * Available anonymous memory.
+ */
+static unsigned long get_available_anon_mem(void)
+{
+	return global_node_page_state(NR_ACTIVE_ANON) +
+		global_node_page_state(NR_INACTIVE_ANON);
+}
+
+/*
+ * Compute "available" memory, that is either free memory or memory that can be
+ * reclaimed quickly, adjusted for the presence of swap.
+ */
+static unsigned long get_available_mem_adj(void)
+{
+	/* free_mem is completely unallocated; clean file-backed memory
+	 * (file_mem - dirty_mem) is easy to reclaim, except for the last
+	 * min_filelist_kbytes. totalreserve_pages is the reserve of pages that
+	 * are not available to user space.
+	 */
+	unsigned long raw_free_mem = global_zone_page_state(NR_FREE_PAGES);
+	unsigned long free_mem = raw_free_mem > totalreserve_pages ?
+			raw_free_mem - totalreserve_pages : 0;
+	unsigned long available_mem = free_mem + get_available_file_mem();
+	unsigned long swappable_pages = min_t(unsigned long,
+			get_nr_swap_pages(), get_available_anon_mem());
+	/*
+	 * The contribution of swap is reduced by a factor of
+	 * low_mem_ram_vs_swap_weight.
+	 */
+	return available_mem + swappable_pages / low_mem_ram_vs_swap_weight;
+}
+
+#ifdef CONFIG_SYSFS
+static void low_mem_threshold_notify(void);
+#else
+static void low_mem_threshold_notify(void)
+{
+}
+#endif
+
+/*
+ * Returns TRUE if we are in a low memory state.
+ */
+bool low_mem_check(void)
+{
+	static bool was_low_mem;	/* = false, as per style guide */
+	static atomic_t in_low_mem_check = ATOMIC_INIT(0);
+	/* last observed threshold */
+	static unsigned int low_mem_threshold_last = UINT_MAX;
+	/* Limit logging low memory to once per second. */
+	static DEFINE_RATELIMIT_STATE(low_mem_logging_ratelimit, 1 * HZ, 1);
+
+	/* We declare a low-memory condition when a combination of RAM and swap
+	 * space is low.
+	 */
+	unsigned long available_mem = get_available_mem_adj();
+	/*
+	 * For backwards compatibility with the older margin interface, we will
+	 * trigger the /dev/chromeos-low_mem device when we are below the
+	 * lowest threshold
+	 */
+	bool is_low_mem = available_mem < low_mem_thresholds[0];
+	unsigned int threshold_lowest = UINT_MAX;
+	int i;
+
+	if (!low_mem_margin_enabled)
+		return false;
+
+	if (atomic_read(&in_low_mem_check) || atomic_xchg(&in_low_mem_check, 1))
+		return was_low_mem;
+
+	if (unlikely(is_low_mem && !was_low_mem) &&
+	    __ratelimit(&low_mem_logging_ratelimit)) {
+		pr_info("entering low_mem (avail RAM = %lu kB, avail swap %lu kB, avail file %lu kB, anon mem: %lu kB)\n",
+			available_mem * PAGE_SIZE / 1024,
+			get_nr_swap_pages() * PAGE_SIZE / 1024,
+			get_available_file_mem() * PAGE_SIZE / 1024,
+			get_available_anon_mem() * PAGE_SIZE / 1024);
+	}
+	was_low_mem = is_low_mem;
+
+	if (is_low_mem)
+		low_mem_notify();
+
+	for (i = 0; i < low_mem_threshold_count; i++) {
+		if (available_mem < low_mem_thresholds[i]) {
+			threshold_lowest = i;
+			break;
+		}
+	}
+
+	/* we crossed one or more thresholds */
+	if (unlikely(threshold_lowest < low_mem_threshold_last))
+		low_mem_threshold_notify();
+
+	low_mem_threshold_last = threshold_lowest;
+
+	atomic_set(&in_low_mem_check, 0);
+
+	return is_low_mem;
+}
+
+static int low_mem_notify_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int low_mem_notify_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static __poll_t low_mem_notify_poll(struct file *file, poll_table *wait)
+{
+	/* Update state to reflect any recent freeing. */
+	atomic_set(&low_mem_state, low_mem_check());
+
+	poll_wait(file, &low_mem_wait, wait);
+
+	if (low_mem_margin_enabled && atomic_read(&low_mem_state))
+		return POLLIN;
+
+	return 0;
+}
+
+const struct file_operations low_mem_notify_fops = {
+	.open = low_mem_notify_open,
+	.release = low_mem_notify_release,
+	.poll = low_mem_notify_poll,
+};
+EXPORT_SYMBOL(low_mem_notify_fops);
+
+#ifdef CONFIG_SYSFS
+
+#define LOW_MEM_ATTR(_name)				      \
+	static struct kobj_attribute low_mem_##_name##_attr = \
+		__ATTR(_name, 0644, low_mem_##_name##_show,   \
+		       low_mem_##_name##_store)
+
+static ssize_t low_mem_margin_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	int i;
+	ssize_t written = 0;
+
+	if (!low_mem_margin_enabled || !low_mem_threshold_count)
+		return sprintf(buf, "off\n");
+
+	for (i = 0; i < low_mem_threshold_count; i++)
+		written += sprintf(buf + written, "%lu ",
+			    low_mem_thresholds[i] * PAGE_SIZE / MB);
+	written += sprintf(buf + written, "\n");
+	return written;
+}
+
+static ssize_t low_mem_margin_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int i = 0, consumed = 0;
+	const char *start = buf;
+	char *endp;
+	unsigned long thresholds[LOW_MEM_THRESHOLD_MAX];
+
+	memset(thresholds, 0, sizeof(thresholds));
+	/*
+	 * Even though the API does not say anything about this, the string in
+	 * buf is zero-terminated (as long as count < PAGE_SIZE) because buf is
+	 * a newly allocated zero-filled page.  Most other sysfs handlers rely
+	 * on this too.
+	 */
+	if (strncmp("off", buf, 3) == 0) {
+		pr_info("low_mem: disabling notifier\n");
+		low_mem_margin_enabled = false;
+		return count;
+	}
+	if (strncmp("on", buf, 2) == 0) {
+		pr_info("low_mem: enabling notifier\n");
+		low_mem_margin_enabled = true;
+		return count;
+	}
+	/*
+	 * This takes a space separated list of thresholds in ascending order,
+	 * and a trailing newline is optional.
+	 */
+	while (consumed < count) {
+		if (i >= LOW_MEM_THRESHOLD_MAX) {
+			pr_warn("low-mem: too many thresholds");
+			return -EINVAL;
+		}
+		/* special case for trailing newline */
+		if (*start == '\n')
+			break;
+
+		thresholds[i] = simple_strtoul(start, &endp, 0);
+		if ((endp == start) && *endp != '\n')
+			return -EINVAL;
+
+		/* make sure each is larger than the last one */
+		if (i && thresholds[i] <= thresholds[i - 1]) {
+			pr_warn("low-mem: thresholds not in increasing order: %lu then %lu\n",
+				thresholds[i - 1], thresholds[i]);
+			return -EINVAL;
+		}
+
+		if (thresholds[i] * (MB / PAGE_SIZE) > totalram_pages()) {
+			pr_warn("low-mem: threshold too high\n");
+			return -EINVAL;
+		}
+
+		consumed += endp - start + 1;
+		start = endp + 1;
+		i++;
+	}
+
+	low_mem_threshold_count = i;
+	low_mem_margin_enabled = !!low_mem_threshold_count;
+
+	/* Convert to pages outside the allocator fast path. */
+	for (i = 0; i < low_mem_threshold_count; i++) {
+		low_mem_thresholds[i] = thresholds[i] * (MB / PAGE_SIZE);
+		pr_info("low_mem: threshold[%d] %lu MB\n", i,
+			low_mem_thresholds[i] * PAGE_SIZE / MB);
+	}
+
+	return count;
+}
+LOW_MEM_ATTR(margin);
+
+static ssize_t low_mem_ram_vs_swap_weight_show(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       char *buf)
+{
+	return sprintf(buf, "%u\n", low_mem_ram_vs_swap_weight);
+}
+
+static ssize_t low_mem_ram_vs_swap_weight_store(struct kobject *kobj,
+						struct kobj_attribute *attr,
+						const char *buf, size_t count)
+{
+	int err;
+	unsigned weight;
+
+	err = kstrtouint(buf, 10, &weight);
+	if (err)
+		return -EINVAL;
+
+	/* The special value 0 represents infinity. */
+	low_mem_ram_vs_swap_weight = weight ? -1 : weight;
+	pr_info("low_mem: setting ram weight to %u\n",
+		low_mem_ram_vs_swap_weight);
+	return count;
+}
+LOW_MEM_ATTR(ram_vs_swap_weight);
+
+static ssize_t low_mem_available_show(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      char *buf)
+{
+	unsigned long available_mem = get_available_mem_adj();
+
+	return sprintf(buf, "%lu\n",
+		       available_mem / (MB / PAGE_SIZE));
+}
+
+static ssize_t low_mem_available_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buf, size_t count)
+{
+	return -EINVAL;
+}
+LOW_MEM_ATTR(available);
+
+static struct attribute *low_mem_attrs[] = {
+	&low_mem_margin_attr.attr,
+	&low_mem_ram_vs_swap_weight_attr.attr,
+	&low_mem_available_attr.attr,
+	NULL,
+};
+
+static struct attribute_group low_mem_attr_group = {
+	.attrs = low_mem_attrs,
+	.name = "chromeos-low_mem",
+};
+
+static struct kernfs_node *low_mem_available_dirent;
+
+static void low_mem_threshold_notify(void)
+{
+	if (low_mem_available_dirent)
+		sysfs_notify_dirent(low_mem_available_dirent);
+}
+
+static int __init low_mem_init(void)
+{
+	int err;
+	struct kernfs_node *low_mem_node;
+
+	err = sysfs_create_group(mm_kobj, &low_mem_attr_group);
+	if (err) {
+		pr_err("low_mem: register sysfs failed\n");
+		return err;
+	}
+
+	low_mem_node = sysfs_get_dirent(mm_kobj->sd, "chromeos-low_mem");
+	if (low_mem_node) {
+		low_mem_available_dirent =
+		    sysfs_get_dirent(low_mem_node, "available");
+		sysfs_put(low_mem_node);
+	}
+
+	if (!low_mem_available_dirent)
+		pr_warn("unable to find dirent for \"available\" attribute\n");
+
+	return 0;
+}
+module_init(low_mem_init)
+
+#endif
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 088313a68f30..0dbf19c2e83f 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -68,6 +68,7 @@
 #include <linux/lockdep.h>
 #include <linux/nmi.h>
 #include <linux/psi.h>
+#include <linux/low-mem-notify.h>
 
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
@@ -4291,6 +4292,7 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,
 	 */
 	if (*no_progress_loops > MAX_RECLAIM_RETRIES) {
 		/* Before OOM, exhaust highatomic_reserve */
+    low_mem_notify();
 		return unreserve_highatomic_pageblock(ac, true);
 	}
 
@@ -4738,6 +4740,8 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 		return NULL;
 
 	finalise_ac(gfp_mask, &ac);
+  
+  low_mem_check();
 
 	/*
 	 * Forbid the first pass from falling back to types that fragment
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 7fde5f904c8d..c3f12dd58159 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2292,6 +2292,28 @@ enum scan_balance {
 	SCAN_FILE,
 };
 
+/*
+ * Low watermark used to prevent fscache thrashing during low memory.
+ */
+int min_filelist_kbytes;
+
+/*
+ * Check low watermark used to prevent fscache thrashing during low memory.
+ */
+static int file_is_low(struct lruvec *lruvec)
+{
+  unsigned long size;
+
+  if (!mem_cgroup_disabled())
+    return false;
+
+  size = node_page_state(lruvec_pgdat(lruvec), NR_ACTIVE_FILE);
+  size += node_page_state(lruvec_pgdat(lruvec), NR_INACTIVE_FILE);
+  size <<= (PAGE_SHIFT - 10);
+
+  return size < min_filelist_kbytes;
+}
+
 /*
  * Determine how aggressively the anon and file LRU lists should be
  * scanned.  The relative value of each set of LRU lists is determined
